{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络 (Deep Correlation Model)\n",
    "\n",
    "- 总体思路：在传统使用机器学习等方式进行因子组合优化的方式中，通常将问题定义为回归问题，损失函数通常具有类似MSE的形式。也即\n",
    "$$Loss1=-\\sum_t (y_{t+1} - f(\\bm{x}_{t,i}))^2$$ \n",
    "使用此类损失函数更多拟合的是$y_{t+1}$和$f(\\bm{x}_{t,i})$均值之间的关系而损失了顺序信息。为此，我们考虑寻找信噪比更高的对象进行学习，将优化目标定为IC。在因子合成中，优化IC的目标函数为\n",
    "$$Loss2=-\\sum_t Corr(y_{t+1}, f(\\bm{x}_{t,i}))$$ \n",
    "其将每期的样本看做一个整体，对整体的结果计算相关性作为损失，较单一样本直接加总的损失有更高的信噪比。由于IC描述的是整体样本的相关性，局部可能出现与整体截然相反的分布，比如整体的IC是正值，但在因子值较高的区域，IC是负值。这一结果可能会显著的影响多头选股的效果。为了更好的适应\\textbf{多头选股}的任务，可采用加权的相关系数，根据因子值从高到低采用指数衰减权重\n",
    "\n",
    "$$w_i=\\left(\\frac{1}{2}\\right)^{\\frac{i-1}{n-1}}, \\quad i=1,\\dots,n$$ \n",
    "\n",
    "在上述加权的基础上，Weighted IC计算如下：\n",
    "$$\\mathbb{E}[x|w]=\\sum_i w_i x_i, \\quad \\mathbb{E}[y|w]=\\sum_i w_i y_i$$ \n",
    "$$Var[x|w]=\\mathbb{E}[x^2|w]-\\mathbb{E}[x|w]^2=\\sum_i w_i x_i^2-\\left(\\sum_i w_i x_i\\right)^2$$ \n",
    "$$Var[y|w]=\\mathbb{E}[y^2|w]-\\mathbb{E}[y|w]^2=\\sum_i w_i y_i^2-\\left(\\sum_i w_i y_i\\right)^2$$ \n",
    "$$Cov(x,y|w)=\\sum_i w_i x_i y_i - \\left(\\sum_i w_i x_i\\right)\\left(\\sum_i w_i y_i\\right)$$ \n",
    "$$Corr(x,y|w)=\\frac{Cov(x,y|w)}{\\sqrt{Var[x|w]Var[y|w]}}$$ \n",
    "使用该方法可以使得模型更关注头部（因子值较高时）的相关性。\n",
    "\n",
    "- 网络结构: \n",
    "考虑到输入数据的形式为横截面数据，为减轻过拟合，我们采用类似多层感知机的三层网络结构，每层分别包含一个64/128/64节点的全连接层和批次标准化层，使用ReLU函数激活。\n",
    "\n",
    "- 模型训练时的损失函数计算中，需将模型输出值$\\hat{y}_{t+1,i}=f(\\bm{x}_{t,i})$排序后计算上述加权IC值作为损失函数，从而通过反向传播更新参数，训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# alphas = pd.read_csv(\"data_residbarrarsector.csv\")\n",
    "alphas = pd.read_csv(\"data_cutnorm.csv\")\n",
    "# barra = pd.read_hdf('barrar_risk.h5')\n",
    "base_data = pd.read_csv(\"base_data.csv\").dropna(subset=['adj_ret_p1'])\n",
    "data = pd.merge(alphas, base_data, on=['date', 'cn_code'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas['year'] = alphas['date'] // 10000\n",
    "data = pd.merge(alphas, base_data, on=['date', 'cn_code'], how='inner')\n",
    "data['year'] = data['date'] // 10000\n",
    "alpha_cols = alphas.columns.drop(['cn_code', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Dense, Input\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow import convert_to_tensor\n",
    "\n",
    "# 半衰加权方式\n",
    "def get_halflife_weights(y_pred):\n",
    "    l = y_pred.shape[0]\n",
    "    weights = np.array([np.power(0.5, (i - 1) / (l - 1)) \\\n",
    "        for i in range(l, 0, -1)]) # 半衰加权\n",
    "    y_pred_ranks = np.argsort(np.argsort(y_pred)) # 两次argsort获取rank\n",
    "    weights = weights[y_pred_ranks] / sum(weights) # 归一化\n",
    "    weights = weights.astype('float32')\n",
    "    return convert_to_tensor(weights)\n",
    "\n",
    "# 定义加权IC，使用keras后端实现\n",
    "def weighted_ic(y_true, y_pred):\n",
    "    weights = get_halflife_weights(y_pred)\n",
    "    mean_true = K.sum(y_true * weights)\n",
    "    mean_pred = K.sum(y_pred * weights)\n",
    "\n",
    "    var_true = K.sum(K.square(y_true) * weights) - \\\n",
    "        K.square(K.sum(y_true * weights))\n",
    "    var_pred = K.sum(K.square(y_pred) * weights) - \\\n",
    "        K.square(K.sum(y_pred * weights))\n",
    "    \n",
    "    cov = K.sum(weights * y_true * y_pred) - mean_true * mean_pred\n",
    "    corr = cov / (K.sqrt(var_pred) * K.sqrt(var_true))\n",
    "    return -corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Dense, Input, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "K.clear_session() # 清除先前训练的模型\n",
    "\n",
    "# 模型结构\n",
    "def MODEL():\n",
    "    input_size = 32 # 输入因子个数，本研究为32个\n",
    "    input_layer = Input(input_size) #输入层\n",
    "    x = input_layer # 继承输入层\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x) #张量拉直\n",
    "    x = Dense(1)(x)\n",
    "    output_layer = x #输出层\n",
    "    model = Model(input_layer, output_layer) #模型组装\n",
    "    # model.summary() #模型细节展示\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Train Year 2020\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 77s 410ms/step - loss: -0.0051 - weighted_ic: -0.0050 - val_loss: -0.0072 - val_weighted_ic: -0.0079\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 225s 1s/step - loss: -0.0124 - weighted_ic: -0.0122 - val_loss: -0.0073 - val_weighted_ic: -0.0079\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 131s 699ms/step - loss: -0.0167 - weighted_ic: -0.0165 - val_loss: -0.0070 - val_weighted_ic: -0.0076\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 93s 497ms/step - loss: -0.0207 - weighted_ic: -0.0207 - val_loss: -0.0068 - val_weighted_ic: -0.0074\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 74s 395ms/step - loss: -0.0245 - weighted_ic: -0.0244 - val_loss: -0.0063 - val_weighted_ic: -0.0069\n",
      "18188/18188 [==============================] - 78s 4ms/step\n",
      "IC For Predicted Alpha in 2021: 0.0052\n",
      "--------------------------------------------------------\n",
      "For Train Year 2021\n",
      "Epoch 1/5\n",
      "195/195 [==============================] - 95s 486ms/step - loss: -0.0016 - weighted_ic: -1.6819e-06 - val_loss: -0.0025 - val_weighted_ic: -0.0026\n",
      "Epoch 2/5\n",
      "195/195 [==============================] - 151s 777ms/step - loss: -0.0119 - weighted_ic: -0.0106 - val_loss: -0.0029 - val_weighted_ic: -0.0030\n",
      "Epoch 3/5\n",
      "195/195 [==============================] - 79s 404ms/step - loss: -0.0161 - weighted_ic: -0.0157 - val_loss: -0.0030 - val_weighted_ic: -0.0031\n",
      "Epoch 4/5\n",
      "195/195 [==============================] - 125s 643ms/step - loss: -0.0187 - weighted_ic: -0.0204 - val_loss: -0.0036 - val_weighted_ic: -0.0037\n",
      "Epoch 5/5\n",
      "195/195 [==============================] - 183s 938ms/step - loss: -0.0212 - weighted_ic: -0.0225 - val_loss: -0.0042 - val_weighted_ic: -0.0043\n",
      "18170/18170 [==============================] - 80s 4ms/step\n",
      "IC For Predicted Alpha in 2022: 0.0041\n",
      "--------------------------------------------------------\n",
      "For Train Year 2022\n",
      "Epoch 1/5\n",
      "194/194 [==============================] - 83s 430ms/step - loss: 9.2341e-04 - weighted_ic: 9.1799e-04 - val_loss: 0.0024 - val_weighted_ic: 0.0024\n",
      "Epoch 2/5\n",
      "194/194 [==============================] - 104s 537ms/step - loss: -0.0050 - weighted_ic: -0.0050 - val_loss: 0.0089 - val_weighted_ic: 0.0089\n",
      "Epoch 3/5\n",
      "194/194 [==============================] - 44s 227ms/step - loss: -0.0101 - weighted_ic: -0.0101 - val_loss: 0.0118 - val_weighted_ic: 0.0118\n",
      "Epoch 4/5\n",
      "194/194 [==============================] - 89s 461ms/step - loss: -0.0142 - weighted_ic: -0.0142 - val_loss: 0.0141 - val_weighted_ic: 0.0141\n",
      "Epoch 5/5\n",
      "194/194 [==============================] - 122s 630ms/step - loss: -0.0178 - weighted_ic: -0.0178 - val_loss: 0.0157 - val_weighted_ic: 0.0157\n",
      "1498/1498 [==============================] - 17s 12ms/step\n",
      "IC For Predicted Alpha in 2023: -0.0153\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 按年滚动训练\n",
    "for train_year in range(2015, 2023):\n",
    "    print(\"For Train Year {:d}\".format(train_year))\n",
    "    # 取出训练和测试样本，按年为划分单位\n",
    "    x_train = data[alpha_cols][data.year == train_year].drop(columns='year').astype('float32')\n",
    "    y_train = data.drop(columns=['date', 'cn_code'])[data.year == train_year].drop(columns='year')['adj_ret_p1'].astype('float32')\n",
    "\n",
    "    x_test = data[alpha_cols][data.year == train_year + 1].drop(columns='year').astype('float32')\n",
    "    y_test = data.drop(columns=['date', 'cn_code'])[data.year == train_year + 1].drop(columns='year')['adj_ret_p1'].astype('float32')\n",
    "\n",
    "    # 清除先前模型\n",
    "    K.clear_session()\n",
    "    # 构建模型\n",
    "    model = MODEL()\n",
    "    # 使用Adam优化器进行训练，损失函数为Weighted IC\n",
    "    model.compile(optimizer = Adam(0.0001),\n",
    "              loss = weighted_ic,\n",
    "              metrics = [weighted_ic],\n",
    "              run_eagerly=True)\n",
    "    # 训练模型，设置batch_size为3000（抽取约一天的股池样本），训练5个epoch\n",
    "    model.fit(x_train, y_train,\n",
    "            validation_data = (x_test, y_test),\n",
    "            batch_size = 3000,\n",
    "            epochs = 5)\n",
    "\n",
    "    # 获得外样本预测结果，计算IC\n",
    "    y_pred_test = model.predict(x_test).reshape(1, -1)[0]\n",
    "    print(\"IC For Predicted Alpha in {:d}: {:.4f}\".format(train_year + 1, np.corrcoef(y_pred_test, y_test)[0][1]))\n",
    "    # 保存结果\n",
    "    if(train_year == 2015):\n",
    "        pred_results = y_pred_test\n",
    "    else:\n",
    "        pred_results = np.hstack([pred_results, y_pred_test])\n",
    "    print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述只展示了2020-2022年数据的训练结果，其余年份训练结果类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.86695486,  0.32839933, -0.5326828 , ..., -2.0387452 ,\n",
       "        0.15744704,  2.5687876 ], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加日期和股票代码\n",
    "pred_results_df = data[data.year >= 2014][['date', 'cn_code']].reset_index().drop(columns='index')\n",
    "pred_results_df['weighted_ic_nn'] = pred_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出结果\n",
    "pred_results_df.to_hdf(\"alpha_aggregations_weighted_ic_nn.h5\", key='stage', mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "406fc061deb33f0aa77d26ddb85a341d574dd3281c000087856ef3a4cde589f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
